Source :   https://hadoop.apache.org/docs/r2.7.5/hadoop-project-dist/hadoop-common/SingleCluster.html





sudo apt-get install ssh
sudo apt-get install rsync


Download hadoop from    -->>>  http://mirrors.estointernet.in/apache/hadoop/common/hadoop-2.7.7/      --->>  hadoop-2.7.7.tar.gz 
(from mirror site)

Unpack the downloaded Hadoop distribution. 
In the extracted distribution, edit the file etc/hadoop/hadoop-env.sh to define some parameters as follows:

a)  first of all find your java path
              readlink -f /usr/bin/java | sed "s:bin/java::"   ====>> results like this ==>> /usr/lib/jvm/java-11-openjdk-amd64/
==>> so go to under that directory and copy all things and paste it under  /usr/java/latest..

==>> if you have not any directory for paste create new one.

b) Now change line in .sh file:
             export JAVA_HOME=/usr/java/latest
             
             
Come under extracted hadoop distribution diredctory. Now type all commands.
c) bin/hadoop

mkdir input
cp etc/hadoop/*.xml input
bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.5.jar grep input output 'dfs[a-z.]+'
cat output/*




                                          Use the following:

etc/hadoop/core-site.xml:

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>







etc/hadoop/hdfs-site.xml:

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

             
             
ssh localhost






Format the filesystem:
      $ bin/hdfs namenode -format


Start NameNode daemon and DataNode daemon:
      $ sbin/start-dfs.sh



 
The hadoop daemon log output is written to the $HADOOP_LOG_DIR directory (defaults to $HADOOP_HOME/logs).
Browse the web interface for the NameNode; by default it is available at:
        NameNode - http://localhost:50070/



Make the HDFS directories required to execute MapReduce jobs:
      $ bin/hdfs dfs -mkdir /user
      $ bin/hdfs dfs -mkdir /user/<username>

    
    
Copy the input files into the distributed filesystem:
      $ bin/hdfs dfs -put etc/hadoop input

   
Run some of the examples provided:
      $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.5.jar grep input output 'dfs[a-z.]+'

    
    
Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them:
      $ bin/hdfs dfs -get output output
      $ cat output/*

    or

View the output files on the distributed filesystem:
      $ bin/hdfs dfs -cat output/*

    
    
When youâ€™re done, stop the daemons with:
      $ sbin/stop-dfs.sh

